"""
Author: Mengdie Huang (Maggie)
Address: Xidian University, Purdue University 
Date: 2024-06-11
@copyright
"""

import torch
from utils.parseargs import parse_argu
from utils.eval import evaluate_model
import os
from models.model import TARFAModel
from dataset.datapro import get_source_dataloader, get_surrogate_dataloader, get_align_surrogate_dataloader, get_filter_source_dataloader, get_target_dataloader
from train.finetune import finetune
from train.stdtrain import stdtrain
from attack.genae import genae

if __name__ == '__main__':
    
    print("======== Maggie is the best! ========\n")
    
    if torch.cuda.is_available():
        print('Torch cuda is available')
        print(f'Total number of GPUs available: {torch.cuda.device_count()}')
        print(f'Total number of CPUs available: {os.cpu_count()}')
        """ 
        Total number of GPUs available: 3
        Total number of CPUs available: 48
        """
    else:
        print("Torch cuda is not available.")
        print(f'Total number of CPUs available: {os.cpu_count()}')
        
        """
        Torch cuda is not available.
        Total number of CPUs available: 48
        """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    args =parse_argu()
    print('args=%s' % args)          
    print('args.save_path=%s' % args.save_path)
 
    device_ids = []
    if args.n_gpus == 2:
        device_ids = [1,2]
    elif args.n_gpus == 3:
        device_ids = [0,1,2]
    print(f"Using GPUs: {device_ids}")

    if args.taskmode == 'pretrain_source':
        print(f"taskmode: {args.taskmode}")
        SourceModel = TARFAModel(args)
        
        _, _, source_test_loader = get_source_dataloader(args)
        SourceModel.testloader = source_test_loader

        if args.n_gpus > 1:
            SourceModel.model = torch.nn.DataParallel(SourceModel.model, device_ids=device_ids)
                             
        print(f"Evaluate the pretrain source model {args.source_model} top1 and top5 accuracy on {args.source_dataset} test set ...")
        metrics = evaluate_model(SourceModel.model, SourceModel.testloader, SourceModel.criterion)
        print(f'Top1 Accuracy on {args.source_dataset} test set: {metrics[0] * 100:.3f}%')
        print(f'Top5 Accuracy on {args.source_dataset} test set: {metrics[1] * 100:.3f}%')
        print(f'Average Loss on {args.source_dataset} test set: {metrics[2]:.3f}')
        print(f"Evaluation Time: {metrics[3]:.3f} seconds")    
    
    if args.taskmode == 'finetune_surrogate':
        print(f"taskmode: {args.taskmode}")
        SurrogateModel = TARFAModel(args)

        surrogate_train_loader, surrogate_val_loader, surrogate_test_loader = get_surrogate_dataloader(args=args, clean=True)
    
        SurrogateModel.trainloader = surrogate_train_loader
        SurrogateModel.valloader = surrogate_val_loader
        SurrogateModel.testloader = surrogate_test_loader
        
        SurrogateModel.num_classes = len(set([label for _, label in SurrogateModel.testloader.dataset]))
        print("SurrogateModel.num_classes:", SurrogateModel.num_classes)
        """
        args.surrogate_dataset from imagenette
        SurrogateModel.num_classes: 10       
        
        args.surrogate_dataset from tiny_imagenet
        SurrogateModel.num_classes: 200
        """            
        
        if args.n_gpus > 1:
            SourceModel.model = torch.nn.DataParallel(SourceModel.model, device_ids=device_ids)      

        finetune_time = finetune(SurrogateModel.model, SurrogateModel.trainloader, SurrogateModel.valloader, SurrogateModel.criterion, SurrogateModel.num_classes, args)
            
        print(f"Evaluate the finetuned surrogate model {args.surrogate_model} top1 and top5 accuracy on {args.surrogate_dataset} test set ...")        
        
        metrics = evaluate_model(SurrogateModel.model, SurrogateModel.testloader, SurrogateModel.criterion)
        print(f'Top1 Accuracy on {args.surrogate_dataset} test set: {metrics[0] * 100:.3f}%')
        print(f'Top5 Accuracy on {args.surrogate_dataset} test set: {metrics[1] * 100:.3f}%')
        print(f'Average Loss on {args.surrogate_dataset} test set: {metrics[2]:.3f}')
        print(f"Evaluation Time: {metrics[3]:.3f} seconds") 
        print(f'Finetune time on {args.surrogate_dataset} training set: {finetune_time:.3f}')

        os.makedirs(f'result/savemodel/surrogatemodel/{args.surrogate_dataset}/{args.surrogate_model}', exist_ok=True)
        model_savepath = f'result/savemodel/surrogatemodel/{args.surrogate_dataset}/{args.surrogate_model}/finetuned-{args.surrogate_model}-{args.surrogate_dataset}-{args.finetune_mode}-top1acc-{metrics[0] * 100:.3f}-top5acc-{metrics[1] * 100:.3f}.hdf5'
        torch.save(SurrogateModel.model, model_savepath)
                                                      
    if args.taskmode == 'surrogate_genae':
        
        print(f"taskmode: {args.taskmode}")
        SurrogateModel = TARFAModel(args)

        surrogate_train_loader, surrogate_val_loader, surrogate_test_loader = get_surrogate_dataloader(args=args, clean=True)
    
        SurrogateModel.trainloader = surrogate_train_loader
        SurrogateModel.valloader = surrogate_val_loader
        SurrogateModel.testloader = surrogate_test_loader
        
        SurrogateModel.num_classes = len(set([label for _, label in SurrogateModel.testloader.dataset]))
        print("SurrogateModel.num_classes:", SurrogateModel.num_classes)
        """
        args.surrogate_dataset from imagenette
        SurrogateModel.num_classes: 10       
        
        args.surrogate_dataset from tiny_imagenet
        SurrogateModel.num_classes: 200
        """            
        
        if args.n_gpus > 1:
            SourceModel.model = torch.nn.DataParallel(SourceModel.model, device_ids=device_ids)   

        #---------test surrogate model on clean surrogate dataset---------
        metrics = evaluate_model(SurrogateModel.model, SurrogateModel.testloader, SurrogateModel.criterion)
        print(f'Top1 Accuracy on {args.surrogate_dataset} test set: {metrics[0] * 100:.3f}%')
        print(f'Top5 Accuracy on {args.surrogate_dataset} test set: {metrics[1] * 100:.3f}%')
        print(f'Average Loss on {args.surrogate_dataset} test set: {metrics[2]:.3f}')
        print(f"Evaluation Time: {metrics[3]:.3f} seconds") 
        
        #---------generate adv surrogate dataset based surrogate model---------        
        print(f"surrogate-{args.surrogate_model}-generate {args.attack_type}-{args.ae_eps}-{args.ae_step}-{args.ae_ite} adversarial-{args.surrogate_dataset}-examples")
        
        adv_testloader, genrate_time = genae(SurrogateModel.model, SurrogateModel.testloader, SurrogateModel.criterion, SurrogateModel.num_classes, args)

        #---------test surrogate model on adv surrogate dataset---------        
        print(f"Evaluate the trained surrogate model {args.surrogate_model} top1 and top5 accuracy on {args.attack_type} adversarial {args.surrogate_dataset} ...")
        metrics = evaluate_model(SurrogateModel.model, adv_testloader, SurrogateModel.criterion)
        print(f'Top1 Accuracy on {args.surrogate_dataset} adv test set: {metrics[0] * 100:.3f}%')
        print(f'Top5 Accuracy on {args.surrogate_dataset} adv test set: {metrics[1] * 100:.3f}%')
        print(f'Average Loss on {args.surrogate_dataset} adv test set: {metrics[2]:.3f}')
        print(f"Evaluation Time: {metrics[3]:.3f} seconds")      
        print(f'Generate time on {args.surrogate_dataset} test set: {genrate_time:.3f}')
        
        #---------save adv surrogate dataset---------                
        all_images = []
        all_labels = []
        for images, labels in adv_testloader:
            all_images.append(images)
            all_labels.append(labels)
        all_images = torch.cat(all_images, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        adv_save_path = f'/home/newdrive/huan1932/data/TARFA-Result/surrogate_genae/{args.surrogate_model}/{args.surrogate_dataset}/'
        os.makedirs(adv_save_path, exist_ok=True)
        
        torch.save(all_images, f'{adv_save_path}/{args.surrogate_model}-{args.surrogate_dataset}-{args.attack_type}-{args.ae_eps}-{args.ae_step}-{args.ae_ite}-top1acc-{metrics[0] * 100:.3f}-top5acc-{metrics[1] * 100:.3f}-adv-test-images.pt')
        torch.save(all_labels, f'{adv_save_path}/{args.surrogate_model}-{args.surrogate_dataset}-{args.attack_type}-{args.ae_eps}-{args.ae_step}-{args.ae_ite}-top1acc-{metrics[0] * 100:.3f}-top5acc-{metrics[1] * 100:.3f}-adv-test-labels.pt')
        
    if args.taskmode == 'transfer_attack':
        print(f"taskmode: {args.taskmode}")
        SourceModel = TARFAModel(args)
        
        if args.n_gpus > 1:
            SourceModel.model = torch.nn.DataParallel(SourceModel.model, device_ids=device_ids)

        print(f"Evaluate the pretrained source model {args.source_model} top1 and top5 accuracy on clean {args.surrogate_dataset} examples ...")

        if args.surrogate_dataset == 'imagenet-200':
            _, _, source_test_loader = get_source_dataloader(args)
            ImageNet_200_test_loader = get_filter_source_dataloader(source_test_loader, args, 'tiny_imagenet')
            print("len(ImageNet_200_test_loader.dataset):",len(ImageNet_200_test_loader.dataset))
            """ 
            len(ImageNet_200_test_loader.dataset): 10000
            """
            SourceModel.testloader = ImageNet_200_test_loader
        elif args.surrogate_dataset == 'imagenet-10-in':
            _, _, source_test_loader = get_source_dataloader(args)
            ImageNet_10_IN_test_loader = get_filter_source_dataloader(source_test_loader, args, 'imagenette')
            print("len(ImageNet_10_IN_test_loader.dataset):",len(ImageNet_10_IN_test_loader.dataset))
            """ 
            len(ImageNet_10_IN_test_loader.dataset): 500
            """
            SourceModel.testloader = ImageNet_10_IN_test_loader     
        elif args.surrogate_dataset == 'imagenet-10-iw':
            _, _, source_test_loader = get_source_dataloader(args)
            ImageNet_10_IW_test_loader = get_filter_source_dataloader(source_test_loader, args, 'imagewoof')
            print("len(ImageNet_10_IW_test_loader.dataset):",len(ImageNet_10_IW_test_loader.dataset))
            """ 
            len(ImageNet_10_IW_test_loader.dataset): 500
            """
            SourceModel.testloader = ImageNet_10_IW_test_loader            
        else: 
            surrogate_train_loader_cle, surrogate_val_loader_cle, surrogate_test_loader_cle = get_align_surrogate_dataloader(args=args, clean=True)
            print("len(surrogate_test_loader_cle.dataset):",len(surrogate_test_loader_cle.dataset))
            SourceModel.testloader = surrogate_test_loader_cle
            
        print("SampleNum = len(SourceModel.testloader.dataset):",len(SourceModel.testloader.dataset))
        metrics = evaluate_model(SourceModel.model, SourceModel.testloader, SourceModel.criterion)
        print(f'Top1 Accuracy on {args.surrogate_dataset} clean test set: {metrics[0] * 100:.3f}%')
        print(f'Top5 Accuracy on {args.surrogate_dataset} clean test set: {metrics[1] * 100:.3f}%')
        print(f'Average Loss on {args.surrogate_dataset} clean test set: {metrics[2]:.3f}')
        print(f"Evaluation Time: {metrics[3]:.3f} seconds")     
        
        print(f"Evaluate the pretrained source model {args.source_model} top1 and top5 accuracy on adversarial {args.surrogate_dataset} examples ...")
        _, _, surrogate_test_loader_adv = get_align_surrogate_dataloader(args=args, adv=True)
        print("len(surrogate_test_loader_adv.dataset):",len(surrogate_test_loader_adv.dataset))            
        SourceModel.advtestloader = surrogate_test_loader_adv

        print("SampleNum = len(SourceModel.advtestloader.dataset):",len(SourceModel.advtestloader.dataset))
        metrics = evaluate_model(SourceModel.model, SourceModel.advtestloader, SourceModel.criterion)
        print(f'Top1 Accuracy on {args.surrogate_dataset} adv test set: {metrics[0] * 100:.3f}%')
        print(f'Top5 Accuracy on {args.surrogate_dataset} adv test set: {metrics[1] * 100:.3f}%')
        print(f'Average Loss on {args.surrogate_dataset} adv test set: {metrics[2]:.3f}')
        print(f"Evaluation Time: {metrics[3]:.3f} seconds")         
        
        surrogate_model = torch.load(args.surrogate_model_path)
        print("surrogate_model_path:",args.surrogate_model_path)
        print(f"Evaluate the finetuned surrogate model {args.surrogate_model} top1 and top5 accuracy on adversarial {args.surrogate_dataset} examples ...")
        _, _, ori_surrogate_test_loader_adv = get_surrogate_dataloader(args=args, adv=True)
        print("len(ori_surrogate_test_loader_adv.dataset):",len(ori_surrogate_test_loader_adv.dataset))  
        
        metrics = evaluate_model(surrogate_model, ori_surrogate_test_loader_adv, torch.nn.CrossEntropyLoss())
        print(f'Top1 Accuracy on {args.surrogate_dataset} adv test set: {metrics[0] * 100:.3f}%')
        print(f'Top5 Accuracy on {args.surrogate_dataset} adv test set: {metrics[1] * 100:.3f}%')
        print(f'Average Loss on {args.surrogate_dataset} adv test set: {metrics[2]:.3f}')
        print(f"Evaluation Time: {metrics[3]:.3f} seconds")       

        raise Exception("maggie stop")
               
    if args.taskmode == 'adapt_target':
        print(f"taskmode: {args.taskmode}")

        target_train_loader, target_val_loader, target_test_loader = get_target_dataloader(args=args, clean=True)
        target_dataset_num_classes = len(set([label for _, label in target_test_loader.dataset]))
        args.target_dataset_num_classes = target_dataset_num_classes
        print("args.target_dataset_num_classes:", args.target_dataset_num_classes)
        
        TargetModel = TARFAModel(args)
        TargetModel.num_classes = target_dataset_num_classes
        
        if args.n_gpus > 1:
            TargetModel.model = torch.nn.DataParallel(TargetModel.model, device_ids=device_ids)
        
        TargetModel.trainloader = target_train_loader
        TargetModel.valloader = target_val_loader
        TargetModel.testloader = target_test_loader
        
        # TargetModel.num_classes = len(set([label for _, label in TargetModel.testloader.dataset]))
        # print("TargetModel.num_classes:", TargetModel.num_classes)
        """ 
        args.target_dataset is cifar10
        Splitted test dataset size: 10000
        Splitted training dataset size: 40000
        Splitted validation dataset size: 10000
        TargetModel.num_classes: 10
        """
        
        stdtrain_time = stdtrain(TargetModel.model, TargetModel.trainloader, TargetModel.valloader, TargetModel.criterion, TargetModel.num_classes, args)
        print(f"Evaluate the finetuned target model {args.target_model} top1 and top5 accuracy on {args.target_dataset} test set ...")        
        metrics = evaluate_model(TargetModel.model, TargetModel.testloader, TargetModel.criterion)
        print(f'Top1 Accuracy on {args.target_dataset} test set: {metrics[0] * 100:.3f}%')
        print(f'Top5 Accuracy on {args.target_dataset} test set: {metrics[1] * 100:.3f}%')
        print(f'Average Loss on {args.target_dataset} test set: {metrics[2]:.3f}')
        print(f"Evaluation Time: {metrics[3]:.3f} seconds") 
        print(f'Stdtrain time on {args.target_dataset} training set: {stdtrain_time:.3f}')

        os.makedirs(f'result/savemodel/targetmodel/{args.target_dataset}/{args.target_model}', exist_ok=True)
        model_savepath = f'result/savemodel/targetmodel/{args.target_dataset}/{args.target_model}/stdtrained-{args.target_model}-{args.target_dataset}-top1acc-{metrics[0] * 100:.3f}-top5acc-{metrics[1] * 100:.3f}.hdf5'
        torch.save(TargetModel.model, model_savepath)        
           
    print("\n")
